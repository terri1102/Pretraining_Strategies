# Pretraining_Strategies

Don't stop pretraining 논문에서 영감을 받아 다양한 언어 모델의 pre-training 방법들을 공부하고 구현해보는 레포지토리입니다.

## 1. Language Model
다음 토큰을 예측하는 방식으로 pre-train합니다.

## 2. Masked Language Model
일부 토큰을 [MASK] 토큰으로 바꾸거나 다른 토큰으로 바꾼 후 원래 토큰을 예측하는 방식으로 pre-train합니다.

## 3. Masked Language Model Variations ()
